{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de1f2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jisoo\\anaconda3\\envs\\virtualenv_python36\\lib\\site-packages\\requests\\__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "sys.path.append('../')\n",
    "mecab = Mecab('C:\\mecab\\mecab-ko-dic')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Train Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 2017년 NTIS 데이터 중 len(x)>256 이상만 학습에 사용\n",
    "\n",
    "path = r'D:\\data\\ICT 트렌드 분석 DATA\\NTIS'\n",
    "\n",
    "files = glob(os.path.join(path, '*.xlsx'))\n",
    "\n",
    "# 오래걸림\n",
    "for i,file in enumerate(files):\n",
    "    data = pd.read_excel(file, engine='openpyxl')\n",
    "    if i>=1:\n",
    "        data2 = pd.read_excel(file, engine='openpyxl')\n",
    "        data = pd.concat([data,data2])\n",
    "\n",
    "data = data.fillna('')\n",
    "\n",
    "len_cut = 256\n",
    "\n",
    "data = data[data['요약문_연구내용'].apply(lambda x: len(x) > len_cut)]\n",
    "doc_list = data['요약문_연구내용'].to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc71c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    doublespace_pattern = re.compile('\\s+')\n",
    "    text = str(text)\n",
    "    text = doublespace_pattern.sub(' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be37c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_path = r'D:\\notebooks\\kisdi\\lda_100\\ldamodel'\n",
    "ldamodel = LdaModel.load(ldamodel_path)\n",
    "train_data = [normalize_text(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a40942",
   "metadata": {},
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c3fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from topic_transformer import TopicTransformer_LSTM, datasets\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "if not os.path.isdir('./log'):\n",
    "    os.makedirs('./log')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# dataloader\n",
    "dataset = datasets.SentenceLabelDataset(dic_path='C:\\mecab\\mecab-ko-dic',\n",
    "                                        train_docs=train_data,\n",
    "                                        lda_model=ldamodel,\n",
    "                                        num_topics=100)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
    "# topictransformer\n",
    "model = TopicTransformer_LSTM(output_dim = 100,\n",
    "                              transformer_model_name='xlm-roberta-base',\n",
    "                              lstm_num_layers=2,\n",
    "                              lstm_hidden_size=512)\n",
    "\n",
    "# optimizer, scheduler\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch:0.95**epoch)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# freeze xlm-r layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze only head layers (downstream mlp)\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('lstm') or name.startswith('fc'):\n",
    "        param.requires_grad = True\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# train\n",
    "total_loss = 0\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features, labels = batch\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss_value = model.loss(model(list(features)), labels)\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_value.item()\n",
    "    total_loss /= len(dataloader)\n",
    "    writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "    print('Epoch {} total loss : {:.5f}'.format(epoch, total_loss))\n",
    "    scheduler.step()\n",
    "\n",
    "    print('================================')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "76473896",
   "metadata": {},
   "source": [
    "# model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d5ed3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'topictransformer.pt')\n",
    "\n",
    "model.load_state_dict(torch.load('topictransformer.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "virtualenv_python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
