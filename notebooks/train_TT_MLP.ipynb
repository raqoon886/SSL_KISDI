{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de1f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "sys.path.append('../')\n",
    "mecab = Mecab('C:\\mecab\\mecab-ko-dic')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Train Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 2017년 NTIS 데이터 중 len(x)>256 이상만 학습에 사용\n",
    "\n",
    "path = r'D:\\data\\ICT 트렌드 분석 DATA\\NTIS'\n",
    "\n",
    "files = glob(os.path.join(path, '*.xlsx'))\n",
    "\n",
    "# 오래걸림\n",
    "for i,file in enumerate(files):\n",
    "    data = pd.read_excel(file, engine='openpyxl')\n",
    "    if i>=1:\n",
    "        data2 = pd.read_excel(file, engine='openpyxl')\n",
    "        data = pd.concat([data,data2])\n",
    "\n",
    "data = data.fillna('')\n",
    "\n",
    "len_cut = 256\n",
    "\n",
    "data = data[data['요약문_연구내용'].apply(lambda x: len(x) > len_cut)]\n",
    "doc_list = data['요약문_연구내용'].to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc71c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    doublespace_pattern = re.compile('\\s+')\n",
    "    text = str(text)\n",
    "    text = doublespace_pattern.sub(' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be37c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_path = r'D:\\notebooks\\kisdi\\lda_100\\ldamodel'\n",
    "ldamodel = LdaModel.load(ldamodel_path)\n",
    "train_data = [normalize_text(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a40942",
   "metadata": {},
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c3fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from topic_transformer import TopicTransformer_MLP, datasets\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "if not os.path.isdir('./log'):\n",
    "    os.makedirs('./log')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# dataloader\n",
    "dataset = datasets.SentenceLabelDataset(dic_path='C:\\mecab\\mecab-ko-dic',\n",
    "                                        train_docs=train_data,\n",
    "                                        lda_model=ldamodel,\n",
    "                                        num_topics=100)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch:   5%|▌         | 1/20 [07:10<2:16:16, 430.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 total loss : 0.00114\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 1/20 [08:36<2:43:26, 516.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-14-c7c58b2c7e96>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 48\u001B[1;33m         \u001B[0mtotal_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss_value\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     49\u001B[0m     \u001B[0mtotal_loss\u001B[0m \u001B[1;33m/=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataloader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m     \u001B[0mwriter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_scalar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"loss\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# topictransformer\n",
    "model = TopicTransformer_MLP(output_dim = 100,\n",
    "                             transformer_model_name='xlm-roberta-base',\n",
    "                             num_head_layers=2,\n",
    "                             head_hidden_dims=[512,256])\n",
    "\n",
    "# optimizer, scheduler\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch:0.95**epoch)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# freeze xlm-r layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze only head layers (downstream mlp)\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('head_layers'):\n",
    "        param.requires_grad = True\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# train\n",
    "total_loss = 0\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features, labels = batch\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss_value = model.loss(model(list(features)), labels)\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_value.item()\n",
    "    total_loss /= len(dataloader)\n",
    "    writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "    print('Epoch {} total loss : {:.5f}'.format(epoch, total_loss))\n",
    "    scheduler.step()\n",
    "\n",
    "    print('================================')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "76473896",
   "metadata": {},
   "source": [
    "# model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'topictransformer.pt')\n",
    "\n",
    "model.load_state_dict(torch.load('topictransformer.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "virtualenv_python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
