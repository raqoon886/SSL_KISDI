{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de1f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "from konlpy.tag import Mecab\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "mecab = Mecab('C:\\mecab\\mecab-ko-dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ce8b2",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0983ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017년 NTIS 데이터 중 len(x)>256 이상만 학습에 사용\n",
    "\n",
    "path = r'D:\\data\\ICT 트렌드 분석 DATA\\NTIS'\n",
    "\n",
    "files = glob(os.path.join(path, '*.xlsx'))\n",
    "\n",
    "# 오래걸림\n",
    "for i,file in enumerate(files):\n",
    "    data = pd.read_excel(file, engine='openpyxl')\n",
    "    if i>=1:\n",
    "        data2 = pd.read_excel(file, engine='openpyxl')\n",
    "        data = pd.concat([data,data2])\n",
    "\n",
    "data = data.fillna('')\n",
    "\n",
    "len_cut = 256\n",
    "\n",
    "data = data[data['요약문_연구내용'].apply(lambda x: len(x) > len_cut)]\n",
    "\n",
    "# sampled_data = data.sample(1000)\n",
    "\n",
    "sample_list = data['요약문_연구내용'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc71c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    doublespace_pattern = re.compile('\\s+')\n",
    "    text = str(text)\n",
    "    text = doublespace_pattern.sub(' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac52339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 110308/110308 [01:49<00:00, 1011.71it/s]\n"
     ]
    }
   ],
   "source": [
    "stopwords = []\n",
    "train_data = []\n",
    "for sample in tqdm(sample_list):\n",
    "    a = normalize_text(sample)\n",
    "    noun_list = [noun for noun in mecab.nouns(a) if len(noun)>1 and noun not in stopwords]\n",
    "    train_data.append(noun_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15466075",
   "metadata": {},
   "source": [
    "# make LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc8927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(train_data)\n",
    "\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in train_data]\n",
    "num_topics = 100\n",
    "ldamodel = LdaModel(corpus, num_topics=num_topics, id2word=dictionary,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbb3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "lda_save_path = r'D:\\notebooks\\kisdi\\lda_100\\ldamodel'\n",
    "\n",
    "ldamodel.save(lda_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c342d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = [' '.join(i) for i in train_data]\n",
    "\n",
    "sample_list = [normalize_text(doc) for doc in sample_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a40942",
   "metadata": {},
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c3fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5829335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, train_docs, corpus, lda_model, num_topics=20):\n",
    "        self.train_docs = train_docs\n",
    "        self.num_topics = num_topics\n",
    "        \n",
    "        self.labels = []\n",
    "        for doc in range(len(train_docs)):\n",
    "            probabilities = [b for (a,b) in lda_model.get_document_topics(corpus[doc], minimum_probability=1e-5)]\n",
    "            if len(probabilities)<self.num_topics:\n",
    "                continue\n",
    "            self.labels.append(probabilities)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.train_docs[idx]\n",
    "        label = torch.Tensor(self.labels[idx])\n",
    "\n",
    "        return doc, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd544900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicTransformer(nn.Module):\n",
    "    def __init__(self, output_dim, transformer_model=None, transformer_model_name=None, max_length=128):\n",
    "        super(TopicTransformer, self).__init__()\n",
    "        \n",
    "        \n",
    "        if transformer_model==None and transformer_model_name==None:\n",
    "            print(\"ERROR : Cannot Load Transformer Model\")\n",
    "            return -1\n",
    "        if transformer_model != None:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(transformer_model.config._name_or_path)\n",
    "            self.transformer_model = transformer_model\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "            self.transformer_model = AutoModel.from_pretrained(transformer_model_name)\n",
    "        \n",
    "        self.hidden_dim = self.transformer_model.config.hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.head_layer = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def forward(self, input_x, device='cuda:0'):\n",
    "        \n",
    "        # Non-Tokenized Input\n",
    "        if type(input_x) == list or type(input_x) == tuple :\n",
    "            tokenized_sentence_list = self.tokenizer(input_x, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt').to(device)\n",
    "        else:\n",
    "            tokenized_sentence_list = input_x.to(device)\n",
    "            \n",
    "        # Transformer forward\n",
    "        x = self.transformer_model(**tokenized_sentence_list).last_hidden_state\n",
    "        \n",
    "        # Avg Pooling\n",
    "        pooling_mask = tokenized_sentence_list.attention_mask.unsqueeze(-1).expand(x.size()).float()\n",
    "        sum_mask = pooling_mask.sum(1)\n",
    "        x = (x*pooling_mask).sum(1) / sum_mask\n",
    "        \n",
    "        # Topic Head\n",
    "        x = F.relu(x)\n",
    "        x = self.head_layer(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return 100 * F.mse_loss(pred, label)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f700da",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# dataloader\n",
    "dataset = CustomDataset(sample_list, corpus, ldamodel, num_topics=100)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "# topictransformer\n",
    "model = TopicTransformer(output_dim = 100,\n",
    "                         transformer_model_name = 'xlm-roberta-base')\n",
    "\n",
    "# optimizer, scheduler\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch:0.95**epoch)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# layer freezing\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# no_freeze = list(model.state_dict().keys())[-6:]\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if name in no_freeze:\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# train\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    training_steps = 0\n",
    "    for i,batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        features, labels = batch\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss_value = model.loss(model(list(features)), labels)\n",
    "        if i==0:\n",
    "            print(\"Epoch {} first loss value : {}\".format(epoch, loss_value.item()))\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss_value.item()\n",
    "    total_loss /= len(dataloader)\n",
    "    print('Epoch {} total loss : {:.5f}'.format(epoch, total_loss))\n",
    "    scheduler.step()\n",
    "\n",
    "    training_steps += 1\n",
    "    global_step += 1\n",
    "\n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92c2ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "torch.save(model.state_dict(), 'tt_20_100topics_freeze.pt')\n",
    "# model.load_state_dict(torch.load('topictransformer.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9a8c16",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1e45a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_docs, num_topics=100):\n",
    "        self.test_docs = test_docs\n",
    "        self.num_topics = num_topics\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_docs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.test_docs[idx]\n",
    "\n",
    "        return doc, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4297631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TopicTransformer(output_dim = 100,\n",
    "                         transformer_model_name = 'xlm-roberta-base')\n",
    "model.load_state_dict(torch.load('tt_20_100topics_freeze.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "346ab78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scopus evaluate\n",
    "scopus_path = r'D:\\data\\ICT 트렌드 분석 DATA_2\\SCOPUS'\n",
    "scopus_data = pd.read_csv(os.path.join(scopus_path, 'SCOPUS ABSTRACT(2017).csv'), encoding='ISO-8859-1')\n",
    "\n",
    "test_list = []\n",
    "for i,a in scopus_data.iterrows():\n",
    "    test_list.append(normalize_text(a['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b0f1998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{¨Ï 2013 IEEE.This paper proposes a novel semiautomatic system domain data analysis method. The method is based on the iterative acquisition and analysis of a large body of bibliometric data, generation of domain taxonomies, and creation of domain models. The method was applied on a smart grid case study through collection and analysis of more than 6000 documents. We have found that our method produces domain models of comparable quality to the traditional manually produced domain models in a more cost-effective way.}}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8d828c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(test_list[:16], device='cpu')\n",
    "pred = pred.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac160c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{¨Ï 2013 IEEE.This paper proposes a novel semiautomatic system domain data analysis method. The method is based on the iterative acquisition and analysis of a large body of bibliometric data, generation of domain taxonomies, and creation of domain models. The method was applied on a smart grid case study through collection and analysis of more than 6000 documents. We have found that our method produces domain models of comparable quality to the traditional manually produced domain models in a more cost-effective way.}}\n",
      "argmax : 10, value : 0.060139868408441544\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.To minimize costs, a buying firm would seek sources which offer a more affordable price for the required products. On the basis of a principal-agent framework, this paper presents a buyer's supplier switching model under asymmetric information to minimize the buying cost considering the volume-dependent switching cost, the competitive reactions and economies of scale effects of the incumbent supplier. The proposed model is first converted into an optimal control problem. Then the optimal supplier switching strategy and the corresponding transfer payment are obtained by virtue of Pontryagin's maximum principle. It is shown that the switching cost components and competitive reactions have significant impacts on the switching decision. Only if the maximum price discount is greater than the fixed component of the switching cost, there may exist a partial switching strategy for the buyer to benefit from the competitive effects. Otherwise, the buyer should take an all-or-nothing switching strategy or no switching strategy. Some managerial implications for sourcing strategies with respect to the competitive reactions and economies of scale effects are provided. Furthermore, we propose a revenue sharing contract to highlight the advantage of the contract designed based on the principle-agent theory. Finally, we employ numerical examples to account for the proposed methods.}}\n",
      "argmax : 62, value : 0.046043943613767624\n",
      "\n",
      "================================\n",
      "{¨Ï 2016 IEEE.The hospital system is a complex service system in which patients and hospitals interact and make their decisions based on bounded rationality and information. In this paper, we develop a generative agent-based model to simulate the behavior of a hospital service system. Our model combines agent-based simulation and queueing models to mimic the hospital service processes. Our goal is to simulate and understand the growth and size distribution of hospitals. This simulation model includes agents for supply elements (i.e., hospitals with different resources and expansion strategies) and demand elements (i.e., patients with different preferences for their hospital selections) in a hospital service system. Three important questions are investigated: 1) what is the emergent size distribution of hospitals? 2) what key factors influence the size distribution? and 3) how sensitive is the size distribution to these key factors? Simulation results show that the size distribution is neither power law nor lognormal. Rather, the distribution is leptokurtic, and more skewed than normal but less skewed than lognormal. This result contradicts those in extant literature on the size distributions of human and natural systems, including cities, firms, power-grids, and citation network. We conduct a set of experiments to identify the generative mechanisms for the hospital size distribution and to test the robustness of the results. The model was validated empirically by using a U.S. hospital size dataset.}}\n",
      "argmax : 75, value : 0.08006471395492554\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.This paper is divided into three parts. First, it introduces the use of random sets for reliability assessment of components with rare failure events. The proposed approach is based on the use of operations defined in the random set framework (expectations, confidence intervals, etc.) to obtain upper and lower bounds and confidence intervals of components reliability without assuming any prior distribution about their lifetimes. Then, instead of using failure probabilities calculated directly from each component's observation in order to obtain system reliability, we propose to construct pseudo-system observations directly from components observations in order to obtain the interval system reliability. Finally, the proposed approach is applied on the evaluation of reliability of large-scale systems with very large fault trees and censored reliability data by using Monte Carlo resampling procedure. A comparison with classical probabilistic approaches is also done.}}\n",
      "argmax : 75, value : 0.07599435746669769\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.Scheduling a multicluster tool with wafer residency time constraints is highly challenging yet important in ensuring high productivity of wafer fabrication. This paper presents a method to find an optimal one-wafer cyclic schedule for it. A Petri net is developed to model the dynamic behavior of the tool. By this model, a schedule of the system is analytically expressed as a function of robots' waiting time. Based on this model, this paper presents the necessary and sufficient conditions under which a feasible one-wafer cyclic schedule exists. Then, it gives efficient algorithms to find such a schedule that is optimal. These algorithms require determining the robots' waiting time via simple calculation and thus are efficient. Examples are given to show the application and effectiveness of the proposed method.}}\n",
      "argmax : 79, value : 0.05089142918586731\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.Semantic memory plays a critical role in reasoning and decision making. It enables an agent to abstract useful knowledge learned from its past experience. Based on an extension of fusion adaptive resonance theory network, this paper presents a novel self-organizing memory model to represent and learn various types of semantic knowledge in a unified manner. The proposed model, called fusion adaptive resonance theory for multimemory learning, incorporates a set of neural processes, through which it may transfer knowledge and cooperate with other long-term memory systems, including episodic memory and procedural memory. Specifically, we present a generic learning process, under which various types of semantic knowledge can be consolidated and transferred from the specific experience encoded in episodic memory. We also identify and formalize two forms of memory interactions between semantic memory and procedural memory, through which more effective decision making can be achieved. We present experimental studies, wherein the proposed model is used to encode various types of semantic knowledge in different domains, including a first-person shooting game called Unreal Tournament, the Toads and Frogs puzzle, and a strategic game known as StarCraft Broodwar. Our experiments show that the proposed knowledge transfer process from episodic memory to semantic memory is able to extract useful knowledge to enhance the performance of decision making. In addition, cooperative interaction between semantic knowledge and procedural skills can lead to a significant improvement in both learning efficiency and performance of the learning agents.}}\n",
      "argmax : 75, value : 0.15063241124153137\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.In network systems with a huge number of nodes, it is not possible to apply input signals to all network nodes to control them. In this paper, we show that this issue can be addressed by designing a network topology so that the nodes in the network system are controllable by a few nodes in the system. A theoretical framework that provides the basic link between structural controllability of network systems and the topology design problem is developed. The results also shed light on how new nodes can be added to the network system without having to introduce new control nodes. Hence, the results are useful in dealing with topology design to obtain a controllable network. Moreover, the results also show under what circumstances a network system with multiple identical nodes is uncontrollable. In many applications, groups of identical nodes are connected to each other which is called network of groups. Here, we address the structural controllability problem for multiple groups of network systems which provides information on proper topology design at both network level (i.e., interconnection of groups) and node level (i.e., interconnection of nodes within a group).}}\n",
      "argmax : 41, value : 0.043921440839767456\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.We propose a lightweight and fast learning algorithm for classifying the features of an unknown terrain that a robot is navigating in. Most of the existing research on unknown terrain classification by mobile robots relies on a single powerful classifier to correctly identify the terrain using sensor data from a single sensor like laser or camera. In contrast, our proposed approach uses multiple modalities of sensed data and multiple, weak but less-complex classifiers for classifying the terrain types. The classifiers are combined using an ensemble learning algorithm to improve the algorithm's training rate as compared to an individual classifier. Our algorithm was tested with data collected by navigating a four-wheeled, autonomous robot, called Explorer, over different terrains including brick, grass, rock, sand, and concrete. Our results show that our proposed approach performs better with up to 63% better prediction accuracy for some terrains as compared to a support vector machine (SVM)-based learning technique that uses sensor data from a single sensor. Despite using multiple classifiers, our algorithm takes only a fraction (1/65) of the time on average, as compared to the SVM technique.}}\n",
      "argmax : 75, value : 0.09338496625423431\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.For a network of dynamical systems coupled via an undirected weighted tree, we consider the problem of which system to apply control, in the case when only a single system receives control. We abstract this problem into a study of eigenvalues of a perturbed Laplacian matrix. We show that this eigenvalue problem has a complete solution for arbitrarily large control by showing that the best and the worst places to apply control have well-known characterization in graph theory, thus linking the computational eigenvalue problem with graph-theoretical concepts. Some partial results are proved in the case when the control effort is bounded. In particular, we show that a local maximum in localizing the best place for control is also a global maximum. We conjecture in the bounded control case that the best place to apply control must also necessarily be a characteristic vertex and present evidence from numerical experiments to support this conjecture.}}\n",
      "argmax : 41, value : 0.09492235630750656\n",
      "\n",
      "================================\n",
      "{¨Ï 2016 IEEE.The solving of nonlinear equation systems (e.g., complex transcendental dispersion equation systems in waveguide systems) is a fundamental topic in science and engineering. Davidenko method has been used by electromagnetism researchers to solve time-invariant nonlinear equation systems (e.g., the aforementioned transcendental dispersion equation systems). Meanwhile, Zhang dynamics (ZD), which is a special class of neural dynamics, has been substantiated as an effective and accurate method for solving nonlinear equation systems, particularly time-varying nonlinear equation systems. In this paper, Davidenko method is compared with ZD in terms of efficiency and accuracy in solving time-invariant and time-varying nonlinear equation systems. Results reveal that ZD is a more competent approach than Davidenko method. Moreover, discrete-time ZD models, corresponding block diagrams, and circuit schematics are presented to facilitate the convenient implementation of ZD by researchers and engineers for solving time-invariant and time-varying nonlinear equation systems online. The theoretical analysis and results on Davidenko method, ZD, and discrete-time ZD models are also discussed in relation to solving time-varying nonlinear equation systems.}}\n",
      "argmax : 41, value : 0.09841088205575943\n",
      "\n",
      "================================\n",
      "{¨Ï 2016 IEEE.A schedule-based system is a system that operates on or contains within a schedule of events and breaks at particular time intervals. Entities within the system show presence or absence in these events by entering or exiting the locations of the events. Given radio frequency identification (RFID) data from a schedule-based system, what can we learn about the system (the events and entities) through data mining? Which data mining methods can be applied so that one can obtain rich actionable insights regarding the system and the domain? The research goal of this paper is to answer these posed research questions, through the development of a framework that systematically produces actionable insights for a given schedule-based system. We show that through integrating appropriate data mining methodologies as a unified framework, one can obtain many insights from even a very simple RFID dataset, which contains only very few fields. The developed framework is general, and is applicable to any schedule-based system, as long as it operates under certain basic assumptions. The types of insights are also general, and are formulated in this paper in the most abstract way. The applicability of the developed framework is illustrated through a case study, where real world data from a schedule-based system is analyzed using the introduced framework. Insights obtained include the profiling of entities and events, the interactions between entity and events, and the relations between events.}}\n",
      "argmax : 75, value : 0.07337824255228043\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.The high-technology industry is capital-, technology-, and knowledge-intensive and considerably emphasizes the speed of innovations. Continual updates and upgrades to technology are required for maintaining competitiveness. Research and development (RD) personnel are core personnel in high-technology industries, but they are insufficient in number. Reducing RD personnel turnover has become a key topic in the human resource management of innovative enterprises. Numerous studies have verified that low job satisfaction weakens employee morale and organizational commitment, leading to a high turnover rate. To examine the assessment criteria for the job satisfaction of RD personnel in high-technology industries, fuzzy theory and decision making trial and evaluation laboratory (DEMATEL) were combined into a novel fuzzy-DEMATEL model. First, fuzzy theory was used to examine job satisfaction criteria and fuzzy semantic analysis. Second, the fuzzy-DEMATEL model was used to calculate causal relationships and the degree of influence among all of the criteria. Finally, a model for assessing the job satisfaction of RD personnel in high-technology industries was developed. The findings of this paper are theoretically innovative and practically applicable to the high-technology industry.}}\n",
      "argmax : 33, value : 0.30719664692878723\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.With various emerging mobile devices, the visual content have be to resized into different sizes or aspect ratios for good viewing experiences. In this paper, we propose a new multioperator retargeting algorithm by using four retargeting operators of seam carving, cropping, warping, and scaling iteratively. To determine which retargeting operator should be used at each iteration, we adopt structural similarity (SSIM) to evaluate the similarity between the original and retargeted images. The retargeting operator sequence is constructed based on the four types of retargeting operators by an optimization process. Since the sizes of original and retargeted images are different, scale-invariant feature transform flow is used for dense correspondence between the original and retargeted images for similarity evaluation. Additionally, visual saliency is used to weight SSIM results based on the characteristics of the human visual system. Experimental results on a public image retargeting database have shown the promising performance of the proposed multioperator retargeting algorithm.}}\n",
      "argmax : 41, value : 0.06741496175527573\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.This paper established the vector Razumikhin-type theorem on exponential stability for stochastic functional differential systems by the stochastic analysis techniques and the property of M-Matrix. Several novel stability criteria with vector L-operator differential inequalities for stochastic delay differential systems, especially some of which includes the cross-item, were obtained by means of the vector Razumikhin theorem. By applying these new results, the robustness of global exponential stability of delay recurrent neural networks to random disturbed has been analyzed.}}\n",
      "argmax : 37, value : 0.042860083281993866\n",
      "\n",
      "================================\n",
      "{¨Ï 2013 IEEE.Design optimization of an unmanned underwater vehicle (UUV) is a complex and a computationally expensive exercise that requires the identification of optimal vehicle dimensions offering the best tradeoffs between the objectives, while satisfying the set of design constraints. Although hull form optimization of marine vessels has long been an active area of research, limited attempts in the past have focused on the design optimization of UUVs and there are even fewer reports on the use of high-fidelity analysis methods within the course of optimization. While it is understood that the high-fidelity analysis is more accurate, they also tend to be far more computationally expensive. Thus, it is important to identify when a high-fidelity analysis is required as opposed to a low-fidelity estimate. The work reported in this paper is an extension of the authors previous work of a design optimization framework, where the design problem was solved using a low-fidelity model based on empirical estimates of drag. In this paper, the framework is extended to deal with high-fidelity estimates derived through seamless integration of computer-aided design, meshing and computational fluid dynamics analysis tools i.e., computer aided 3-D interactive application, ICEM, and FLUENT. The effects of using low-fidelity and high-fidelity analyses are studied in depth using a small-scale (length nominally less than 400 mm) and light-weight (less than 450 g) toy submarine. Useful insights on possible means to identify appropriateness of fidelity models via correlation measures are proposed. The term optimality used in this paper refers to optimal hull form shapes that satisfy placement of a set of prescribed internal components.}}\n",
      "argmax : 62, value : 0.04735294356942177\n",
      "\n",
      "================================\n",
      "{¨Ï 2016 IEEE.This paper studies the distributed robust fusion estimation problem with stochastic and deterministic parameter uncertainties, where the covariance of the Gaussian white noise is unknown, and the covariances of the random variables in the stochastic uncertainties are in a bounded set. By using the discrete-time stochastic bounded real lemma and the matrix analysis approach, each local robust estimator is derived to guarantee an optimal estimation performance for admissible uncertainties, and then necessary and sufficient condition for the distributed robust fusion estimator is presented to obtain an optimal weighting fusion criterion. Note that the local robust estimation problem and the distributed robust fusion estimation problem are both converted into convex optimization problems, which can be easily solved by standard software packages. The advantage and effectiveness of the proposed methods are demonstrated through state monitoring for target tracking system and stirred rank reactor system.}}\n",
      "argmax : 41, value : 0.06370003521442413\n",
      "\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "for idx,i in enumerate(pred):\n",
    "    print(f\"{test_list[idx]}\")\n",
    "    print(f\"argmax : {i.argmax().item()+1}, value : {i.max().item()}\\n\")\n",
    "    print('================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2963ee",
   "metadata": {},
   "source": [
    "## lda infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "876bc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dictionary.load(r'./ldamodel_100.id2word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19a4bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_inp = '''{IEEE 2013.노드 수가 많은 네트워크 시스템에서는 모든 네트워크 노드에 입력 신호를 적용하여 제어할 수 없다. 본 논문에서, 우리는 네트워크 시스템의 노드가 시스템의 몇 개의 노드에 의해 제어될 수 있도록 네트워크 토폴로지를 설계함으로써 이 문제를 해결할 수 있음을 보여준다. 네트워크 시스템의 구조적 제어 가능성과 토폴로지 설계 문제 사이의 기본적인 연결을 제공하는 이론적 프레임워크가 개발되었다. 결과는 또한 새로운 제어 노드를 도입하지 않고도 네트워크 시스템에 새로운 노드를 추가할 수 있는 방법을 조명한다. 따라서, 결과는 제어 가능한 네트워크를 얻기 위해 위상 설계를 다루는 데 유용하다. 또한 결과는 동일한 노드가 여러 개 있는 네트워크 시스템이 어떤 상황에서 제어 불가능한지도 보여준다. 많은 응용 프로그램에서 동일한 노드의 그룹이 서로 연결되며 이를 그룹 네트워크라고 합니다. 여기서는 네트워크 수준(즉, 그룹의 상호 연결)과 노드 수준(즉, 그룹 내 노드의 상호 연결) 모두에서 적절한 토폴로지 설계에 대한 정보를 제공하는 여러 네트워크 시스템 그룹에 대한 구조적 제어 가능성 문제를 해결한다.}}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e2718b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.15137996\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.058793165\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.17392823\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.15347907\n",
      "0.021342337\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.06901116\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.041816887\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.045624018\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.2050952\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.05890894\n",
      "0.000107548265\n",
      "0.011049239\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n",
      "0.000107548265\n"
     ]
    }
   ],
   "source": [
    "nouns = [i for i in mecab.nouns(lda_inp) if len(i)>1]\n",
    "\n",
    "tmp = dictionary.doc2bow(nouns)\n",
    "\n",
    "print('\\n'.join([str(a) for i,a in (ldamodel.get_document_topics(tmp, minimum_probability=1e-5))]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "virtualenv_python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "426.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
