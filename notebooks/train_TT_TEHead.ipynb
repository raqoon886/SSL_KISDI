{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de1f2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jisoo\\anaconda3\\envs\\virtualenv_python36\\lib\\site-packages\\requests\\__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from glob import glob\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "sys.path.append('../')\n",
    "mecab = Mecab('C:\\mecab\\mecab-ko-dic')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Train Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 2017년 NTIS 데이터 중 len(x)>256 이상만 학습에 사용\n",
    "\n",
    "path = r'D:\\data\\ICT 트렌드 분석 DATA\\NTIS'\n",
    "\n",
    "files = glob(os.path.join(path, '*.xlsx'))\n",
    "\n",
    "# 오래걸림\n",
    "for i,file in enumerate(files):\n",
    "    data = pd.read_excel(file, engine='openpyxl')\n",
    "    if i>=1:\n",
    "        data2 = pd.read_excel(file, engine='openpyxl')\n",
    "        data = pd.concat([data,data2])\n",
    "\n",
    "data = data.fillna('')\n",
    "\n",
    "len_cut = 256\n",
    "\n",
    "data = data[data['요약문_연구내용'].apply(lambda x: len(x) > len_cut)]\n",
    "doc_list = data['요약문_연구내용'].to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc71c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    doublespace_pattern = re.compile('\\s+')\n",
    "    text = str(text)\n",
    "    text = doublespace_pattern.sub(' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be37c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_path = r'D:\\notebooks\\kisdi\\lda_100\\ldamodel'\n",
    "ldamodel = LdaModel.load(ldamodel_path)\n",
    "train_data = [normalize_text(doc) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a40942",
   "metadata": {},
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c3fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from topic_transformer import TopicTransformer_TEHead, datasets\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "if not os.path.isdir('./log'):\n",
    "    os.makedirs('./log')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# dataloader\n",
    "dataset = datasets.SentenceLabelDataset(dic_path='C:\\mecab\\mecab-ko-dic',\n",
    "                                        train_docs=train_data,\n",
    "                                        lda_model=ldamodel,\n",
    "                                        num_topics=100)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Epoch:   5%|▌         | 1/20 [11:57<3:47:03, 717.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 total loss : 0.00103\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 2/20 [23:34<3:31:42, 705.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 total loss : 0.00079\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 3/20 [35:13<3:19:03, 702.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 total loss : 0.00070\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 4/20 [46:59<3:07:39, 703.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 total loss : 0.00064\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "# topictransformer\n",
    "model = TopicTransformer_TEHead(output_dim = 100,\n",
    "                             transformer_model_name='xlm-roberta-base')\n",
    "\n",
    "# optimizer, scheduler\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch:0.95**epoch)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "#\n",
    "# # freeze xlm-r layers\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "#\n",
    "# # unfreeze only head layers (downstream mlp)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if name.startswith('head_layers'):\n",
    "#         param.requires_grad = True\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# train\n",
    "total_loss = 0\n",
    "for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features, labels = batch\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        loss_value = model.loss(model(list(features),device='cuda:0',ptm_freeze=True), labels)\n",
    "        loss_value.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_value.item()\n",
    "    total_loss /= len(dataloader)\n",
    "    writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "    print('Epoch {} total loss : {:.5f}'.format(epoch, total_loss))\n",
    "    scheduler.step()\n",
    "\n",
    "    print('================================')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "76473896",
   "metadata": {},
   "source": [
    "# model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d5ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'TT_MLP_epoch40.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TopicTransformer_MLP:\n\tMissing key(s) in state_dict: \"head_layers.0.weight\", \"head_layers.0.bias\", \"head_layers.1.weight\", \"head_layers.1.bias\", \"head_layers.2.weight\", \"head_layers.2.bias\", \"head_layers.3.weight\", \"head_layers.3.bias\". \n\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-45a6f482f43c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'TT_MLP_epoch40.pt'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'topictransformer.pt'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\anaconda3\\envs\\virtualenv_python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mload_state_dict\u001B[1;34m(self, state_dict, strict)\u001B[0m\n\u001B[0;32m   1481\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror_msgs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1482\u001B[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001B[1;32m-> 1483\u001B[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001B[0m\u001B[0;32m   1484\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_IncompatibleKeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmissing_keys\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munexpected_keys\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1485\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for TopicTransformer_MLP:\n\tMissing key(s) in state_dict: \"head_layers.0.weight\", \"head_layers.0.bias\", \"head_layers.1.weight\", \"head_layers.1.bias\", \"head_layers.2.weight\", \"head_layers.2.bias\", \"head_layers.3.weight\", \"head_layers.3.bias\". \n\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". "
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('topictransformer.pt'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "virtualenv_python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
